import os
from collections import Counter
from file_utils import read_text_file, write_text_file, read_csv_file, write_csv_file, get_files_in_folder
from text_utils import (
    lemmatize_text, count_words, count_unique_lemmas, calculate_ttr, find_longest_word,
    get_pos_statistics, get_most_common_lemmas, get_verbs, count_specific_lemmas_unique, lexical_density
)

import nltk
nltk.download('punkt')           # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng') # POS-tagging
nltk.download('wordnet')         # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
nltk.download('stopwords')       # –°—Ç–æ–ø-—Å–ª–æ–≤–∞

# –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
from nltk.corpus import stopwords
russian_stopwords = set(stopwords.words('russian'))

def analyze_single_text(filepath, filename):
    """
    –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∫–æ—Ä–ø—É—Å–µ.
    """
    text = read_text_file(filepath)
    if text.startswith("–û—à–∏–±–∫–∞"):
        return {"filename": filename, "error": text}

    lemmas = lemmatize_text(text)
    pos_stats = get_pos_statistics(text)

    filtered_lemmas = [l for l in lemmas if l not in russian_stopwords]
    top_lemmas = Counter(filtered_lemmas).most_common(10)

    verbs = get_verbs(text)

    return {
        "filename": filename,
        "text": text,
        "words_count": count_words(text),
        "lemmas": lemmas,
        "unique_lemmas": count_unique_lemmas(text),
        "ttr": round(calculate_ttr(text),4),
        "lexical_density": round(lexical_density(text),4),
        "longest_word": find_longest_word(text),
        "lines_count": text.count("\n")+1,
        "pos_stats": pos_stats,
        "top_lemmas": top_lemmas,
        "verbs": verbs
    }


# –ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞
def analyze_corpus(corpus_folder):
    results = []
    files = get_files_in_folder(corpus_folder, ".txt")
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤.\n")
    for i, fname in enumerate(files,1):
        print(f"[{i}/{len(files)}] –ê–Ω–∞–ª–∏–∑: {fname}")
        filepath = os.path.join(corpus_folder, fname)
        results.append(analyze_single_text(filepath,fname))
    print("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω.\n")
    return results

def generate_report(results, metadata, character_groups, color_words):
    """
    –í—ã–≤–æ–¥–∏—Ç –æ—Ç—á–µ—Ç.
    """
    report_lines = []
    report_lines.append("="*60)
    report_lines.append("–û–¢–ß–Å–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –ö–û–†–ü–£–°–ê")
    report_lines.append("="*60+"\n")

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_texts = len([r for r in results if "error" not in r])
    total_words = sum(r["words_count"] for r in results if "error" not in r)
    all_lemmas = []
    for r in results:
        if "error" not in r:
            all_lemmas.extend(r["lemmas"])
    total_unique_lemmas = len(set(all_lemmas))
    avg_ttr = round(sum(r["ttr"] for r in results if "error" not in r)/total_texts,4) if total_texts else 0

    report_lines.append("–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    report_lines.append(f"  –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {total_texts}")
    report_lines.append(f"  –í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
    report_lines.append(f"  –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {total_unique_lemmas}")
    report_lines.append(f"  –°—Ä–µ–¥–Ω–∏–π TTR: {avg_ttr}\n")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–µ–∫—Å—Ç—É
    report_lines.append("–î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –§–ê–ô–õ–ê–ú:")
    report_lines.append("-"*50)
    metadata_map = {row["filename"]: row for row in metadata}

    for r in results:
        fname = r["filename"]
        meta = metadata_map.get(fname,{})
        title = meta.get("title",fname)
        report_lines.append(f"\nüìÑ {title}")
        report_lines.append(f"   –ê–≤—Ç–æ—Ä: {meta.get('author','?')}")
        report_lines.append(f"   –ì–æ–¥: {meta.get('year','?')}")
        if "error" in r:
            report_lines.append(f"   –û—à–∏–±–∫–∞: {r['error']}")
            continue
        report_lines.append(f"   –°–ª–æ–≤: {r['words_count']}")
        report_lines.append(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–µ–º–º: {r['unique_lemmas']}")
        report_lines.append(f"   TTR: {r['ttr']}")
        report_lines.append(f"   –õ–µ–∫—Å–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å: {r['lexical_density']}")
        report_lines.append(f"   –°–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ: {r['longest_word']}")
        report_lines.append(f"   –°—Ç—Ä–æ–∫: {r['lines_count']}")
        report_lines.append("   –¢–æ–ø-10 –ª–µ–º–º: " + ", ".join([f"{l}:{c}" for l,c in r["top_lemmas"]]))
        report_lines.append("   –ß–∞—Å—Ç–∏ —Ä–µ—á–∏: " + ", ".join([f"{k}:{v}" for k,v in r["pos_stats"].items()]))
        report_lines.append("   5 —Å–∞–º—ã—Ö —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã—Ö –≥–ª–∞–≥–æ–ª–æ–≤: " + ", ".join(r["verbs"][:5]))

    # –í—ã–≤–æ–¥—ã –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ
    report_lines.append("\n–í–´–í–û–î–´ –ò –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø:")

    # –ü–æ–¥—Å—á–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
    report_lines.append("\n–£–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π (–ø–æ —Ç–µ–∫—Å—Ç–∞–º, –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ —Ç–µ–∫—Å—Ç):")
    for char, words in character_groups.items():
        count = sum(max(count_specific_lemmas_unique(r["text"], words).values()) for r in results if "error" not in r)
        report_lines.append(f"  {char}: {count}")

    # –ü–æ–∏—Å–∫ —Å–∞–º–æ–≥–æ –ª–µ–∫—Å–∏—á–µ—Å–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    valid_results = [r for r in results if "error" not in r]
    if valid_results:
        metadata_map = {row["filename"]: row for row in metadata}
        lex_diverse = max(valid_results, key=lambda x: x["ttr"])
        title = metadata_map.get(lex_diverse["filename"], {}).get("title", lex_diverse["filename"])
        report_lines.append(f"\n–°–∞–º—ã–π –ª–µ–∫—Å–∏—á–µ—Å–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–π —Ç–µ–∫—Å—Ç: {title} (TTR={lex_diverse['ttr']})")

    # –ü–æ–∏—Å–∫ –∞–≤—Ç–æ—Ä–∞ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–æ–≤
    authors = [metadata_map.get(r["filename"],{}).get("author","?") for r in results if "error" not in r]
    if authors:
        most_author = Counter(authors).most_common(1)[0]
        report_lines.append(f"–ê–≤—Ç–æ—Ä —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–æ–≤: {most_author[0]} ({most_author[1]})")

    #–ü–æ–∏—Å–∫ —Å–∞–º–æ–≥–æ –¥–ª–∏–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
    all_longest = [r["longest_word"] for r in results if "error" not in r]
    report_lines.append(f"–°–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –ø–æ –∫–æ—Ä–ø—É—Å—É: {max(all_longest, key=len) if all_longest else ''}")

    # –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ—Ä–ø—É—Å—É (—Å —É—á–µ—Ç–æ–º —Ñ–æ—Ä–º —Å–ª–æ–≤–∞ –∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤)
    color_variants = {
        "—á–µ—Ä–Ω—ã–π": ["—á–µ—Ä–Ω—ã–π","—á—ë—Ä–Ω—ã–π","—á–µ—Ä–Ω–æ—Ç–∞"],
        "–±–µ–ª—ã–π": ["–±–µ–ª—ã–π","–±–µ–ª–æ–≤–∞—Ç—ã–π","–±–µ–ª–∏–∑–Ω–∞","–±–µ–ª–æ–≥–æ–ª–æ–≤—ã–π","–±–µ–ª–æ–≤–æ–ª–æ—Å—ã–π"],
        "–∫—Ä–∞—Å–Ω—ã–π": ["–∫—Ä–∞—Å–Ω—ã–π","–∫—Ä–∞—Å–Ω–æ—Ç–∞","–∫—Ä–∞—Å–Ω–æ–≤–∞—Ç—ã–π","–∫—Ä–æ–≤—å","–∫—Ä–æ–≤–∞–≤—ã–π"],
        "—Å–µ—Ä—ã–π": ["–ø–µ–ø–µ–ª—å–Ω—ã–π","–ø–µ–ø–µ–ª","—Å–µ—Ä–µ–±—Ä–æ","—Å–µ—Ä–µ–±—Ä—è–Ω—ã–π", "—Å–µ—Ä—ã–π","—Å—Ç–∞–ª—å","—Å—Ç–∞–ª—å–Ω–æ–π"],
        "–∂–µ–ª—Ç—ã–π": ["–∂–µ–ª—Ç—ã–π","–∑–æ–ª–æ—Ç–æ–π","–∑–æ–ª–æ—Ç–æ"],
        "–∑–µ–ª–µ–Ω—ã–π": ["–∏–∑—É–º—Ä—É–¥–Ω—ã–π","–∏–∑—É–º—Ä—É–¥","–∑–µ–ª–µ–Ω—å","–∑–µ–ª–µ–Ω—ã–π"],
        "—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π": ["—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π","—Ñ–∏–∞–ª–∫–æ–≤—ã–π","—Ñ–∏–∞–ª–∫–∞","—Å–∏—Ä–µ–Ω—å"],
    }

    color_counts = {color: 0 for color in color_words}

    for r in results:
        if "error" in r:
            continue
        lemmas_set = set(r["lemmas"])
        for color, variants in color_variants.items():
            # –µ—Å–ª–∏ –ª—é–±–∞—è –ª–µ–º–º–∞ —Ç–µ–∫—Å—Ç–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ª—é–±—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º —Ü–≤–µ—Ç–∞, —É—á–∏—Ç—ã–≤–∞–µ–º
            if any(var in lemmas_set for var in variants):
                color_counts[color] += 1

    report_lines.append("\n–¶–≤–µ—Ç–æ–≥—Ä–∞–º–º–∞:")
    max_val = max(color_counts.values()) if color_counts else 1
    for color, val in color_counts.items():
        bar = "‚ñà" * int(40*val/max_val) if val > 0 else ""
        report_lines.append(f"{color:12}: {bar} ({val})")

    # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–ø-10 –ª–µ–º–º
    counter = Counter()
    for r in valid_results:
        filtered_lemmas = [l for l in r["lemmas"] if l not in russian_stopwords]
        counter.update(filtered_lemmas)

    report_lines.append("\n10 —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –ª–µ–º–º:")
    for l,c in counter.most_common(10):
        report_lines.append(f"  {l}: {c}")


    # –ü–æ–¥—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
    avg_words = round(sum(r["words_count"] for r in valid_results)/len(valid_results),2) if valid_results else 0
    avg_lines = round(sum(r["lines_count"] for r in valid_results)/len(valid_results),2) if valid_results else 0
    report_lines.append(f"\n–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤): {avg_words}")
    report_lines.append(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å—Ç—Ä–æ–∫): {avg_lines}")

    # –ü–æ–¥—Å—á–µ—Ç —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –≥–ª–∞–≥–æ–ª–æ–≤ (—Ç–æ–ø-5)
    verbs_counter = Counter()
    for r in valid_results:
        verbs_counter.update(r["verbs"])
    report_lines.append("\n5 —Å–∞–º—ã—Ö —É–ø–æ—Ç—Ä–µ–±–ª—è–µ–º—ã—Ö –≥–ª–∞–≥–æ–ª–æ–≤:")
    for v,c in verbs_counter.most_common(5):
        report_lines.append(f"  {v}: {c}")

    # –°–∞–º–∞—è —á–∞—Å—Ç–æ—Ç–Ω–∞—è —á–∞—Å—Ç—å —Ä–µ—á–∏
    pos_counter = Counter()
    for r in valid_results:
        pos_counter.update(r["pos_stats"])
    if pos_counter:
        most_pos = pos_counter.most_common(1)[0]
        report_lines.append(f"\n–°–∞–º–∞—è —á–∞—Å—Ç–æ—Ç–Ω–∞—è —á–∞—Å—Ç—å —Ä–µ—á–∏: {most_pos[0]} ({most_pos[1]})")

    return "\n".join(report_lines)

def main():
    """
    –°–æ–∑–¥–∞–µ—Ç CSV-—Ñ–∞–π–ª –∏ –≤—ã–∑—ã–≤–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
    """
    corpus_folder = "corpus"
    metadata_file = "data/metadata.csv"

    metadata = read_csv_file(metadata_file)
    results = analyze_corpus(corpus_folder)

    character_groups = {
        "–ì–µ—Ä–∞–ª—å—Ç": ["–≤–æ–ª–∫","–±–µ–ª—ã–π","–≥–µ—Ä–∞–ª—å—Ç","–±–µ–ª–æ–≥–æ–ª–æ–≤—ã–π","–±–µ–ª–æ–≤–æ–ª–æ—Å—ã–π","–≤–µ–¥—å–º–∞–∫","–æ—Ö–æ—Ç–Ω–∏–∫","—Å—Ç–∞–ª—å","–º–µ—á"],
        "–¶–∏—Ä–∏": ["–ª–∞—Å—Ç–æ—á–∫–∞","–¥–∏—Ç—è","–¥–µ–≤–æ—á–∫–∞","—Ü–∏—Ä–∏","—Ü–∏—Ä–∏–ª–ª–∞","—Ñ–∞–ª—å–∫–∞","–ø–µ–ø–µ–ª—å–Ω—ã–π","–∑–µ–ª–µ–Ω—ã–π","–±–∞—à–Ω—è"],
        "–ô–µ–Ω–Ω–∏—Ñ—ç—Ä": ["–π–µ–Ω–Ω–∏—Ñ—ç—Ä","–π–µ–Ω–Ω–∏—Ñ–µ—Ä","–π–µ–Ω","—á–∞—Ä–æ–¥–µ–π–∫–∞","—Å–∏—Ä–µ–Ω—å","–∫—Ä—ã–∂–æ–≤–Ω–∏–∫","–Ω–æ—á—å"]
    }

    color_words = ["—á–µ—Ä–Ω—ã–π","–±–µ–ª—ã–π","–∫—Ä–∞—Å–Ω—ã–π","–∂–µ–ª—Ç—ã–π","–∑–µ–ª–µ–Ω—ã–π","—Å–µ—Ä—ã–π","—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π"]

    # CSV
    headers = ["filename","author","year","title","words_count","unique_lemmas","ttr","lexical_density","longest_word","lines_count"]
    csv_rows = []
    meta_map = {row["filename"]: row for row in metadata}
    for r in results:
        m = meta_map.get(r["filename"],{})
        csv_rows.append([
            r["filename"],
            m.get("author",""),
            m.get("year",""),
            m.get("title",""),
            r.get("words_count",""),
            r.get("unique_lemmas",""),
            r.get("ttr",""),
            r.get("lexical_density",""),
            r.get("longest_word",""),
            r.get("lines_count","")
        ])
    os.makedirs("results", exist_ok=True)
    write_csv_file("results/statistics.csv", csv_rows, headers)

    # TXT
    report = generate_report(results, metadata, character_groups, color_words)
    write_text_file("results/report.txt", report)
    print("\n–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ results.\n")

if __name__ == "__main__":
    main()